{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>stateName</th>\n",
       "      <th>countyName</th>\n",
       "      <th>ccvi</th>\n",
       "      <th>theme1</th>\n",
       "      <th>theme2</th>\n",
       "      <th>theme3</th>\n",
       "      <th>theme4</th>\n",
       "      <th>theme5</th>\n",
       "      <th>theme6</th>\n",
       "      <th>theme7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.441972</td>\n",
       "      <td>0.458336</td>\n",
       "      <td>0.377478</td>\n",
       "      <td>0.170811</td>\n",
       "      <td>0.841617</td>\n",
       "      <td>0.966294</td>\n",
       "      <td>0.209805</td>\n",
       "      <td>0.287591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.742619</td>\n",
       "      <td>0.562935</td>\n",
       "      <td>0.584587</td>\n",
       "      <td>0.683826</td>\n",
       "      <td>0.768947</td>\n",
       "      <td>0.982399</td>\n",
       "      <td>0.209805</td>\n",
       "      <td>0.427810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.778072</td>\n",
       "      <td>0.245094</td>\n",
       "      <td>0.645024</td>\n",
       "      <td>0.472226</td>\n",
       "      <td>0.863762</td>\n",
       "      <td>0.953086</td>\n",
       "      <td>0.724817</td>\n",
       "      <td>0.439041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.362727</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.370119</td>\n",
       "      <td>0.192050</td>\n",
       "      <td>0.977978</td>\n",
       "      <td>0.894654</td>\n",
       "      <td>0.209805</td>\n",
       "      <td>0.425573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.319725</td>\n",
       "      <td>0.421440</td>\n",
       "      <td>0.764060</td>\n",
       "      <td>0.753240</td>\n",
       "      <td>0.978307</td>\n",
       "      <td>0.749454</td>\n",
       "      <td>0.497597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72832</th>\n",
       "      <td>56043000200</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>0.193098</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.106362</td>\n",
       "      <td>0.792014</td>\n",
       "      <td>0.669875</td>\n",
       "      <td>0.126091</td>\n",
       "      <td>0.259923</td>\n",
       "      <td>0.008196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72833</th>\n",
       "      <td>56043000301</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>0.288511</td>\n",
       "      <td>0.536803</td>\n",
       "      <td>0.358751</td>\n",
       "      <td>0.539916</td>\n",
       "      <td>0.400654</td>\n",
       "      <td>0.199599</td>\n",
       "      <td>0.259923</td>\n",
       "      <td>0.589599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72834</th>\n",
       "      <td>56043000302</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>0.574666</td>\n",
       "      <td>0.804595</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.613380</td>\n",
       "      <td>0.701398</td>\n",
       "      <td>0.137185</td>\n",
       "      <td>0.815509</td>\n",
       "      <td>0.273972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72835</th>\n",
       "      <td>56045951100</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Weston</td>\n",
       "      <td>0.112481</td>\n",
       "      <td>0.476381</td>\n",
       "      <td>0.238316</td>\n",
       "      <td>0.834326</td>\n",
       "      <td>0.559188</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.037122</td>\n",
       "      <td>0.008224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72836</th>\n",
       "      <td>56045951300</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Weston</td>\n",
       "      <td>0.317812</td>\n",
       "      <td>0.621751</td>\n",
       "      <td>0.291037</td>\n",
       "      <td>0.740620</td>\n",
       "      <td>0.483470</td>\n",
       "      <td>0.234197</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.152109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72182 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              FIPS stateName countyName      ccvi    theme1    theme2  \\\n",
       "0       1001020100   ALABAMA    Autauga  0.441972  0.458336  0.377478   \n",
       "1       1001020200   ALABAMA    Autauga  0.742619  0.562935  0.584587   \n",
       "2       1001020300   ALABAMA    Autauga  0.778072  0.245094  0.645024   \n",
       "3       1001020400   ALABAMA    Autauga  0.362727  0.023280  0.370119   \n",
       "4       1001020500   ALABAMA    Autauga  0.816600  0.319725  0.421440   \n",
       "...            ...       ...        ...       ...       ...       ...   \n",
       "72832  56043000200   WYOMING   Washakie  0.193098  0.632400  0.106362   \n",
       "72833  56043000301   WYOMING   Washakie  0.288511  0.536803  0.358751   \n",
       "72834  56043000302   WYOMING   Washakie  0.574666  0.804595  0.349800   \n",
       "72835  56045951100   WYOMING     Weston  0.112481  0.476381  0.238316   \n",
       "72836  56045951300   WYOMING     Weston  0.317812  0.621751  0.291037   \n",
       "\n",
       "         theme3    theme4    theme5    theme6    theme7  \n",
       "0      0.170811  0.841617  0.966294  0.209805  0.287591  \n",
       "1      0.683826  0.768947  0.982399  0.209805  0.427810  \n",
       "2      0.472226  0.863762  0.953086  0.724817  0.439041  \n",
       "3      0.192050  0.977978  0.894654  0.209805  0.425573  \n",
       "4      0.764060  0.753240  0.978307  0.749454  0.497597  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "72832  0.792014  0.669875  0.126091  0.259923  0.008196  \n",
       "72833  0.539916  0.400654  0.199599  0.259923  0.589599  \n",
       "72834  0.613380  0.701398  0.137185  0.815509  0.273972  \n",
       "72835  0.834326  0.559188  0.149020  0.037122  0.008224  \n",
       "72836  0.740620  0.483470  0.234197  0.442149  0.152109  \n",
       "\n",
       "[72182 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv on covid-19 covid vulnerability index data and convert to dataframe\n",
    "ccvi = pd.read_csv('../resources/ccvi.csv')\n",
    "\n",
    "# drop rows that contain any null values (there are 655 of them)\n",
    "ccvi = ccvi.dropna(how='any')\n",
    "\n",
    "# display dataframe\n",
    "ccvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Cases_Total</th>\n",
       "      <th>Cases_White</th>\n",
       "      <th>Cases_Black</th>\n",
       "      <th>Cases_Latinx</th>\n",
       "      <th>Cases_Asian</th>\n",
       "      <th>Cases_AIAN</th>\n",
       "      <th>Cases_NHPI</th>\n",
       "      <th>Cases_Multiracial</th>\n",
       "      <th>...</th>\n",
       "      <th>Tests_Latinx</th>\n",
       "      <th>Tests_Asian</th>\n",
       "      <th>Tests_AIAN</th>\n",
       "      <th>Tests_NHPI</th>\n",
       "      <th>Tests_Multiracial</th>\n",
       "      <th>Tests_Other</th>\n",
       "      <th>Tests_Unknown</th>\n",
       "      <th>Tests_Ethnicity_Hispanic</th>\n",
       "      <th>Tests_Ethnicity_NonHispanic</th>\n",
       "      <th>Tests_Ethnicity_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210307</td>\n",
       "      <td>AK</td>\n",
       "      <td>59332.0</td>\n",
       "      <td>18300.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2447.0</td>\n",
       "      <td>12238.0</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>4453.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210307</td>\n",
       "      <td>AL</td>\n",
       "      <td>499819.0</td>\n",
       "      <td>160347.0</td>\n",
       "      <td>82790.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2273.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210307</td>\n",
       "      <td>AR</td>\n",
       "      <td>324818.0</td>\n",
       "      <td>207596.0</td>\n",
       "      <td>50842.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2913.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>3358.0</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210307</td>\n",
       "      <td>AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210307</td>\n",
       "      <td>AZ</td>\n",
       "      <td>826454.0</td>\n",
       "      <td>308453.0</td>\n",
       "      <td>25775.0</td>\n",
       "      <td>244539.0</td>\n",
       "      <td>11921.0</td>\n",
       "      <td>40707.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>20200412</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>20200412</td>\n",
       "      <td>WA</td>\n",
       "      <td>10411.0</td>\n",
       "      <td>2903.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>20200412</td>\n",
       "      <td>WI</td>\n",
       "      <td>3341.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>20200412</td>\n",
       "      <td>WV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>20200412</td>\n",
       "      <td>WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date State  Cases_Total  Cases_White  Cases_Black  Cases_Latinx  \\\n",
       "0     20210307    AK      59332.0      18300.0       1499.0           NaN   \n",
       "1     20210307    AL     499819.0     160347.0      82790.0           NaN   \n",
       "2     20210307    AR     324818.0     207596.0      50842.0           NaN   \n",
       "3     20210307    AS          NaN          NaN          NaN           NaN   \n",
       "4     20210307    AZ     826454.0     308453.0      25775.0      244539.0   \n",
       "...        ...   ...          ...          ...          ...           ...   \n",
       "5315  20200412    VT          NaN          NaN          NaN           NaN   \n",
       "5316  20200412    WA      10411.0       2903.0        289.0        1180.0   \n",
       "5317  20200412    WI       3341.0       1680.0        857.0           NaN   \n",
       "5318  20200412    WV          NaN          NaN          NaN           NaN   \n",
       "5319  20200412    WY          NaN          NaN          NaN           NaN   \n",
       "\n",
       "      Cases_Asian  Cases_AIAN  Cases_NHPI  Cases_Multiracial  ...  \\\n",
       "0          2447.0     12238.0      1508.0             4453.0  ...   \n",
       "1          2273.0         NaN         NaN                NaN  ...   \n",
       "2          2913.0      1070.0      3358.0             1804.0  ...   \n",
       "3             NaN         NaN         NaN                NaN  ...   \n",
       "4         11921.0     40707.0         NaN                NaN  ...   \n",
       "...           ...         ...         ...                ...  ...   \n",
       "5315          NaN         NaN         NaN                NaN  ...   \n",
       "5316        451.0        41.0        61.0              112.0  ...   \n",
       "5317         81.0        28.0         NaN                NaN  ...   \n",
       "5318          NaN         NaN         NaN                NaN  ...   \n",
       "5319          NaN         NaN         NaN                NaN  ...   \n",
       "\n",
       "      Tests_Latinx  Tests_Asian  Tests_AIAN  Tests_NHPI  Tests_Multiracial  \\\n",
       "0              NaN          NaN         NaN         NaN                NaN   \n",
       "1              NaN          NaN         NaN         NaN                NaN   \n",
       "2              NaN          NaN         NaN         NaN                NaN   \n",
       "3              NaN          NaN         NaN         NaN                NaN   \n",
       "4              NaN          NaN         NaN         NaN                NaN   \n",
       "...            ...          ...         ...         ...                ...   \n",
       "5315           NaN          NaN         NaN         NaN                NaN   \n",
       "5316           NaN          NaN         NaN         NaN                NaN   \n",
       "5317           NaN          NaN         NaN         NaN                NaN   \n",
       "5318           NaN          NaN         NaN         NaN                NaN   \n",
       "5319           NaN          NaN         NaN         NaN                NaN   \n",
       "\n",
       "      Tests_Other  Tests_Unknown  Tests_Ethnicity_Hispanic  \\\n",
       "0             NaN            NaN                       NaN   \n",
       "1             NaN            NaN                       NaN   \n",
       "2             NaN            NaN                       NaN   \n",
       "3             NaN            NaN                       NaN   \n",
       "4             NaN            NaN                       NaN   \n",
       "...           ...            ...                       ...   \n",
       "5315          NaN            NaN                       NaN   \n",
       "5316          NaN            NaN                       NaN   \n",
       "5317          NaN            NaN                       NaN   \n",
       "5318          NaN            NaN                       NaN   \n",
       "5319          NaN            NaN                       NaN   \n",
       "\n",
       "      Tests_Ethnicity_NonHispanic  Tests_Ethnicity_Unknown  \n",
       "0                             NaN                      NaN  \n",
       "1                             NaN                      NaN  \n",
       "2                             NaN                      NaN  \n",
       "3                             NaN                      NaN  \n",
       "4                             NaN                      NaN  \n",
       "...                           ...                      ...  \n",
       "5315                          NaN                      NaN  \n",
       "5316                          NaN                      NaN  \n",
       "5317                          NaN                      NaN  \n",
       "5318                          NaN                      NaN  \n",
       "5319                          NaN                      NaN  \n",
       "\n",
       "[5320 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get covid data for each race by state\n",
    "covid = pd.read_csv('../resources/CRDT_Data.csv')\n",
    "\n",
    "# display dataframe\n",
    "covid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for convertying state names to corresponding numbers or abbreviations\n",
    "states = {\n",
    "    'southcarolina': {'num': '45', 'abbr': 'SC'},\n",
    "    'southdakota': {'num': '46', 'abbr': 'SD'},\n",
    "    'tennessee': {'num': '47', 'abbr': 'TN'},\n",
    "    'texas': {'num': '48', 'abbr': 'TX'},\n",
    "    'vermont': {'num': '50', 'abbr': 'VT'},\n",
    "    'utah': {'num': '49', 'abbr': 'UT'},\n",
    "    'virginia': {'num': '51', 'abbr': 'VA'},\n",
    "    'washington': {'num': '53', 'abbr': 'WA'},\n",
    "    'westvirginia': {'num': '54', 'abbr': 'WV'},\n",
    "    'wisconsin': {'num': '55', 'abbr': 'WI'},\n",
    "    'wyoming': {'num': '56', 'abbr': 'WY'},\n",
    "    'puertorico': {'num': '72', 'abbr': 'PR'},\n",
    "    'alabama': {'num': '01', 'abbr': 'AL'},\n",
    "    'alaska': {'num': '02', 'abbr': 'AK'},\n",
    "    'arizona': {'num': '04', 'abbr': 'AZ'},\n",
    "    'arkansas': {'num': '05', 'abbr': 'AR'},\n",
    "    'california': {'num': '06', 'abbr': 'CA'},\n",
    "    'colorado': {'num': '08', 'abbr': 'CO'},\n",
    "    'delaware': {'num': '10', 'abbr': 'CT'},\n",
    "    'districtofcolumbia': {'num': '11', 'abbr': 'DE'},\n",
    "    'connecticut': {'num': '09', 'abbr': 'DC'},\n",
    "    'florida': {'num': '12', 'abbr': 'FL'},\n",
    "    'georgia': {'num': '13', 'abbr': 'GA'},\n",
    "    'idaho': {'num': '16', 'abbr': 'ID'},\n",
    "    'hawaii': {'num': '15', 'abbr': 'HI'},\n",
    "    'illinois': {'num': '17', 'abbr': 'IL'},\n",
    "    'indiana': {'num': '18', 'abbr': 'IN'},\n",
    "    'iowa': {'num': '19', 'abbr': 'IA'},\n",
    "    'kansas': {'num': '20', 'abbr': 'KS'},\n",
    "    'kentucky': {'num': '21', 'abbr': 'KS'},\n",
    "    'louisiana': {'num': '22', 'abbr': 'LA'},\n",
    "    'maine': {'num': '23', 'abbr': 'ME'},\n",
    "    'maryland': {'num': '24', 'abbr': 'MD'},\n",
    "    'massachusetts': {'num': '25', 'abbr': 'MA'},\n",
    "    'michigan': {'num': '26', 'abbr': 'MI'},\n",
    "    'minnesota': {'num': '27', 'abbr': 'MN'},\n",
    "    'mississippi': {'num': '28', 'abbr': 'MS'},\n",
    "    'missouri': {'num': '29', 'abbr': 'MO'},\n",
    "    'montana': {'num': '30', 'abbr': 'MT'},\n",
    "    'nebraska': {'num': '31', 'abbr': 'NE'},\n",
    "    'nevada': {'num': '32', 'abbr': 'NV'},\n",
    "    'newhampshire': {'num': '33', 'abbr': 'NH'},\n",
    "    'newjersey': {'num': '34', 'abbr': 'NJ'},\n",
    "    'newmexico': {'num': '35', 'abbr': 'NM'},\n",
    "    'newyork': {'num': '36', 'abbr': 'NY'},\n",
    "    'northcarolina': {'num': '37', 'abbr': 'NC'},\n",
    "    'northdakota': {'num': '38', 'abbr': 'ND'},\n",
    "    'oregon': {'num': '41', 'abbr': 'OR'},\n",
    "    'pennsylvania': {'num': '42', 'abbr': 'PA'},\n",
    "    'rhodeisland': {'num': '44', 'abbr': 'RI'}\n",
    "}\n",
    "\n",
    "# all statistical categories to to be queried \n",
    "pops = 'B01003_001E,B02001_002E,B02001_003E,B02001_004E,B02001_005E,B02001_006E,B03001_003E'\n",
    "\n",
    "# create list of racial groups to iterate through\n",
    "races = ['total','white','black','native','asian','pacific','hispanic']\n",
    "\n",
    "# dictionary with all data to be used from all states that made the data avaliable\n",
    "stateData = {}\n",
    "\n",
    "# all states without necessary data\n",
    "error_states = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-4a8ca7f482d8>:57: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  demogs[race]['ccvi'] = (ccvi_and_pop[race]*ccvi_and_pop['ccvi']).sum()/demogs[race]['population']\n"
     ]
    }
   ],
   "source": [
    "# iterate through states\n",
    "for state in states:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # get census state number\n",
    "        state_num = states[state]['num']\n",
    "\n",
    "        # create url to request data from api\n",
    "        url = f'https://api.census.gov/data/2019/acs/acs5?get=NAME,{pops}&for=tract:*&in=state:{state_num}'\n",
    "\n",
    "        # set returned data to a variable\n",
    "        response = requests.get(url).json()\n",
    "\n",
    "        # create list to store dictionaries with data for each census tract\n",
    "        tracts = []\n",
    "\n",
    "        # create dictionaries with population data for each census tract \n",
    "        # (with properly formatted fips code)\n",
    "        for r in response:\n",
    "            if r[0] != 'NAME':\n",
    "                tracts.append({\n",
    "                    'FIPS': int(f'{r[8]}{r[9]}{r[10]}'),\n",
    "                    'total': int(r[1]),\n",
    "                    'white': int(r[2]),\n",
    "                    'black': int(r[3]),\n",
    "                    'native': int(r[4]),\n",
    "                    'asian': int(r[5]),\n",
    "                    'pacific': int(r[6]),\n",
    "                    'hispanic': int(r[7])\n",
    "                })\n",
    "\n",
    "        # create dataframe with census population data\n",
    "        populations = pd.DataFrame(tracts)\n",
    "\n",
    "        # merge population data and ccvi data on census tract fips code\n",
    "        ccvi_and_pop = pd.merge(populations, ccvi, on='FIPS')\n",
    "\n",
    "        # create dictionary to hold data for each racial demographic\n",
    "        demogs = {\n",
    "            'total': {},\n",
    "            'white': {},\n",
    "            'black': {},\n",
    "            'native': {},\n",
    "            'asian': {},\n",
    "            'pacific': {},\n",
    "            'hispanic': {}\n",
    "        }\n",
    "\n",
    "        # iterate through list of races\n",
    "        for race in races:\n",
    "\n",
    "            # calculate total population for each race\n",
    "            demogs[race]['population'] = int(ccvi_and_pop[race].sum())\n",
    "\n",
    "            # calculate average ccvi for each race\n",
    "            demogs[race]['ccvi'] = (ccvi_and_pop[race]*ccvi_and_pop['ccvi']).sum()/demogs[race]['population']\n",
    "\n",
    "            # calculate population of each race as a percentage of total population\n",
    "            demogs[race]['population_percent'] = (demogs[race]['population']/demogs['total']['population'])*100\n",
    "\n",
    "        # get covid data for each race by state\n",
    "        covid = pd.read_csv('../resources/CRDT_Data.csv')\n",
    "\n",
    "        # filter to only include data for selected state\n",
    "        covid = covid.loc[covid['State'] == states[state]['abbr'],:]\n",
    "\n",
    "        # filter to only include data from 2020\n",
    "        covid = covid.loc[covid['Date'] < 20210000,:]\n",
    "\n",
    "        # create dataframe with only relevant columns for covid cases\n",
    "        cases = covid[['Cases_Total','Cases_White','Cases_Black','Cases_AIAN','Cases_Asian','Cases_NHPI','Cases_Ethnicity_Hispanic']]\n",
    "\n",
    "        # create dataframe with only relevant columns for covid deaths\n",
    "        deaths = covid[['Deaths_Total','Deaths_White','Deaths_Black','Deaths_AIAN','Deaths_Asian','Deaths_NHPI','Deaths_Ethnicity_Hispanic']]\n",
    "\n",
    "        # iterate through covid data for selected races and place data in a dictionary\n",
    "        for i in range(0, len(cases.columns)):\n",
    "\n",
    "            # total cases for each race\n",
    "            demogs[races[i]]['cases'] = int(cases[cases.columns[i]].values[0])\n",
    "\n",
    "            # number of cases for each race as a percentage of total cases\n",
    "            demogs[races[i]]['percent_of_cases'] = (demogs[races[i]]['cases']/demogs['total']['cases'])*100\n",
    "\n",
    "            # percent discrepancy between percent of total cases and percent of total population for by each race\n",
    "            # (theoretically each race should account for the same percent of cases as their percent of the population)\n",
    "            demogs[races[i]]['discrepancy_percent'] = (demogs[races[i]]['percent_of_cases']/demogs[races[i]]['population_percent'])*100\n",
    "\n",
    "            # total deaths for each race\n",
    "            demogs[races[i]]['deaths'] = int(deaths[deaths.columns[i]].values[0])\n",
    "\n",
    "            # chance of an infection resulting in death for each race\n",
    "            demogs[races[i]]['chance_of_death'] = (demogs[races[i]]['deaths']/demogs[races[i]]['cases'])*100\n",
    "\n",
    "            # number of deaths for each race as a percentage of total deaths\n",
    "            demogs[races[i]]['percent_of_deaths'] = (demogs[races[i]]['deaths']/demogs['total']['deaths'])*100\n",
    "\n",
    "        # create dataframe without total population values\n",
    "        demographics = pd.DataFrame(demogs).drop(columns=['total'])\n",
    "\n",
    "        # create dictionary to hold calculated values to be used in max patch\n",
    "        for_max = {}\n",
    "\n",
    "        # iterate through statistical categories\n",
    "        for row in list(demographics.index):\n",
    "\n",
    "            # create a list that holds all values within the row of a statistical category\n",
    "            values = demographics.loc[row].values\n",
    "\n",
    "            # iterate through races\n",
    "            for i in range(1, len(races)):\n",
    "\n",
    "                # get population numbers\n",
    "                if row == 'population':\n",
    "                    for_max[races[i]] = {}\n",
    "                    for_max[races[i]][row] = int(values[i-1])\n",
    "\n",
    "                # calculate inverted ccvi values\n",
    "                elif row == 'ccvi':\n",
    "                    for_max[races[i]]['inverted_ccvi'] = round(100-(values[i-1])*100, 2)\n",
    "\n",
    "                # calculate chances for where next infection will occur\n",
    "                elif row == 'discrepancy_percent':\n",
    "                    for_max[races[i]]['chance_of_infection'] = round((values[i-1]/values.sum())*100, 2)\n",
    "\n",
    "                # get values for chance of infection resulting in death\n",
    "                elif row == 'chance_of_death':\n",
    "                    for_max[races[i]][row] = round(values[i-1], 2)\n",
    "\n",
    "        # create keys to hold number of cases and deaths generated by Max alogrithm\n",
    "        for key in for_max:\n",
    "            for_max[key]['generated_cases'] = 0\n",
    "            for_max[key]['generated_deaths'] = 0\n",
    "\n",
    "        # add state and its data to stateData dictionary\n",
    "        stateData[state] = for_max\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        # add state to list of states with inadequate data\n",
    "        error_states.append(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tennessee\n",
      "utah\n",
      "washington\n",
      "wyoming\n",
      "alaska\n",
      "arkansas\n",
      "california\n",
      "colorado\n",
      "georgia\n",
      "illinois\n",
      "iowa\n",
      "maine\n",
      "minnesota\n",
      "missouri\n",
      "nebraska\n",
      "oregon\n"
     ]
    }
   ],
   "source": [
    "# display avaliable states\n",
    "for state in stateData.keys():\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southcarolina\n",
      "southdakota\n",
      "texas\n",
      "vermont\n",
      "virginia\n",
      "westvirginia\n",
      "wisconsin\n",
      "puertorico\n",
      "alabama\n",
      "arizona\n",
      "delaware\n",
      "districtofcolumbia\n",
      "connecticut\n",
      "florida\n",
      "idaho\n",
      "hawaii\n",
      "indiana\n",
      "kansas\n",
      "kentucky\n",
      "louisiana\n",
      "maryland\n",
      "massachusetts\n",
      "michigan\n",
      "mississippi\n",
      "montana\n",
      "nevada\n",
      "newhampshire\n",
      "newjersey\n",
      "newmexico\n",
      "newyork\n",
      "northcarolina\n",
      "northdakota\n",
      "pennsylvania\n",
      "rhodeisland\n"
     ]
    }
   ],
   "source": [
    "# display unavaliable states\n",
    "for state in error_states:\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output state data as dictionary\n",
    "with open(\"../resources/stateData.json\", \"w\") as outfile:\n",
    "    json.dump(stateData, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
